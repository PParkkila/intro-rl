\documentclass[12pt]{article}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{todonotes}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{soul} % for HL
\usepackage{color} % for HL

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% \renewcommand{\theenumi}{\roman{enumi}}
\newcommand{\rmn}[1]{{\textcolor{blue}{\bf [{\sc rmn:} #1]}}}
\DeclareMathOperator*{\argmax}{arg\,max}

\usetikzlibrary{positioning,calc}
%%%%%%%%%
\usepackage[most]{tcolorbox}
\newtcolorbox[]{solution}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Solution,
    #1
}

\newtcolorbox[]{fillme}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Fill me in,
    #1
}

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\prob}{\mathbb{P}}
\newcommand*{\F}{\mathcal{F}}
\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

%%%%%%%%%%
\title{ECE433/COS435 Introduction to RL\\
  Assignment 0: Review of Topics\\
  Spring 2024\\
}

\date{Due February 4, 2024}
\author{\begin{fillme}[width=0.3\textwidth]
 Your name here.
\end{fillme}} % Fill in your name!

\begin{document}

\maketitle
\noindent

\section*{Collaborators}
\begin{fillme}
 Please fill in the name and netids of your collaborators in this section.
\end{fillme}

\section*{Instructions}

You should work alone for this this assignment. Writeups should be typeset in Latex and submitted as PDF. You can work with whatever tool you like for the code, but \textbf{please submit the asked-for snippet and answer in the solutions box as part of your writeup. We will only be grading your writeup.} Make sure to still also attach your notebook/code with your submission.

\section*{Introduction}
This Homework is meant to be a review of standard topics on machine learning, linear algebra, and probability, with a heavy emphasis on topics that will reappear in later stages of this course. 
\\\\
You will also work with PyTorch and Python to build your models, so we want to make sure you are familiar with the specific packages (e.g. \texttt{torch.distributions}) that you will be using throughout the course.
\newpage

% ----------------- QUESTION 1 ----------------- %

\section*{Question 1. Probability}
In a reinforcement learning setting, we are often interested in settings where an agent interacts with a simple game. We often want to learn the agent's policy $\pi$, or its ``rule" for making actions. In this problem, you will derive properties of different ``rules" over a simple game, more interesting parts of the course.
\\\\
Suppose we are playing a really simple game with $10$ unique boxes, each with their own distinct label from $1,...,10$. One of these boxes contains a golden coin that awards the player with a reward of $5$, one contains a silver coin that awards the player with a reward of $1$, and the rest contain nothing and award the player with a reward of $0$ if selected. The player must open a single box, and based on the outcome, their final score is the reward corresponding to the box that they selected. More formally, the player has a policy $\pi$ and uses it to select an action $a \sim \pi$ corresponding to the box the player picked (e.g. $a=1$ means the player picked box $1$). The player is then given a reward $r(a)$ based on their action.

\subsection*{Question 1.a}
Suppose our \textbf{agent's policy} is to randomly select one of the labelled boxes,\href{https://en.wikipedia.org/wiki/Discrete_uniform_distribution}{ each with equal probability}. In other words, $\pi = \mathcal{U}\{1,10\}$, and the agent selects a box $a \sim \pi$. Conditioned on the fact the the gold coin is in the box labelled $5$ and the silver coin is in box labeled $1$, what is the expected reward?
\begin{solution}
You solution here...
\end{solution}

\subsection*{Question 1.b}
Given the same assumptions as (1.a), what is the policy of the agent that maximizes the expected reward?
\begin{solution}
You solution here...
\end{solution}

\subsection*{Question 1.c}
What is the entropy of the policy in (1.a) and the entropy of the policy in (1.b)? 

\begin{solution}
 Your solution...
\end{solution}

\subsection*{Question 1.d \textbf{Coding}}
Suppose instead of boxes, we now deal with a \textit{continuous} set of possible decisions. In this scenario, our agent can choose any number $a \in \mathbb{R}$, and the reward given to the player is defined by
$$
    \mathcal{R}(x) = \max \biggr \{ 0, 1 - |x - 1| \biggr\}
$$
In this problem, you will use this reward function to compute the expected return of a policy (to be 
defined). The key idea is that an expectation can be approximated through sampling (\href{https://princeton-introml.github.io/files/ch15.pdf}{recall Monte Carlo sampling from COS324}). In this problem, you will familiarize yourself with the \href{https://pytorch.org/docs/stable/distributions.html}{\texttt{torch.distributions}} library.
\\\\
See the colab notebook (1.d) for more details. Fill 
out the indicated code segments below after solving 
the problem.

\begin{solution}
Expected reward is ... \textbf{FILL ME}
\\\\
\textbf{Fill in your code here:}
\begin{lstlisting}[language=Python]
# Your code here...
r = reward(...)
print("Expected reward:", r.mean())
\end{lstlisting}
\end{solution}

\subsection*{Question 1.e \textbf{Coding}}
Plot the reward distribution under this policy using \texttt{matplotlib.pyplot.hist()} or some other histogram plotting library. Make sure to label your axes for this problem (x-axis is reward, y-axis is probability density). See colab notebook (1.e) for more details.
\\\\
\textit{\textbf{Hint.} When using \texttt{matplotlib.pyplot.hist()}, remember it needs to be a PDF! (check the documentation.)}

\begin{solution}
\begin{lstlisting}[language=Python]
# Your code here...
...
plt.plot()
\end{lstlisting}
\end{solution}
\newpage

% ----------------- QUESTION 2 ----------------- %
\section*{Question 2. MLE }
The purpose of this question is to review Maximum Likelihood Estimation (MLE), which appears in many machine learning settings. In previous courses, students may have seen or solved a few questions on computing the Maximum Likelihood Estimator for a set of datapoints, in which case this problem will be review.
\\\\
A random variable $X$ has a Normal distribution with parameters $\mu,\sigma^2$ if its probability distribution is uniquely defined as
$$ f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( - \frac{(x - \mu)^2}{2\sigma^2} \right)  $$

\subsection*{Question 2.a } 
Let $\theta = \sigma^2$. For a set of datapoints $X_1,...,X_n$ sampled i.i.d. from a Normal distribution with unknown parameters $\mu, \theta$ find the log-likelihood function $L(\mu, \theta; X_1,...,X_n)$. You answer should be in terms of $n$, $X_1,...,X_n$, $\mu$, and $\theta$.

\begin{solution}
    $$ \log L(\mu, \theta; X_1,...,X_n) = ... $$
\end{solution}
\subsection*{Question 2.b} 
Using your answer in (3.a), solve for the MLE of $\mu$ and $\theta$. Solutions should also show that the critical points are maximums e.g. compute derivatives around the critical point and number of critical points argument.
\begin{solution}
    $$ \hat{\mu} = ... $$
    $$ \hat{\theta} = ... $$
\end{solution}
\newpage

% ----------------- QUESTION 3 ----------------- %
\section*{Question 3. Coding }
The objective of this question is to familiarize you with the basic mechanics of PyTorch and Python. See \textbf{Question 3} on the .ipynb notebook, but you will be coding up gradient descent and building a basic neural network model. 

\subsection*{Question 3.a } 
For this problem and (3.b), see the provided .ipynb notebook for all the questions, and fill out the indicated code segments below after solving 
each problem.

\begin{solution}
\begin{lstlisting}[language=Python]
# Your code here...
def gradient_descent(X, y, weights: np.ndarray, eta: float, iterations: int) -> None:
  for i in range(iterations):
    # YOUR CODE HERE!
\end{lstlisting}
\end{solution}

\subsection*{Question 3.b } 
Rewrite your solution to (a) using the torch library. Your solution must use \texttt{loss.backward()} and \texttt{weight.grad}.

\begin{solution}
\begin{lstlisting}[language=Python]
# Your code here...
def gradient_descent_torch(X, y, weights: torch.tensor, 
eta: float, iterations: int) -> None:
  for i in range(iterations):
    # YOUR CODE HERE!
\end{lstlisting}
\end{solution}

% \end{solution}
\newpage

\end{document}


